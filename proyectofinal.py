# -*- coding: utf-8 -*-
"""ProyectoFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vBXruRxr58vuu-y5Fo_GpwCahmcPTpo
"""

# ------------------------------- PROYECTO FINAL: TEMA: SUICIDIOS ------------------------------- #

# Primero, cargaremos todas las librerias necesarias para trabajar en nuestro proyecto final. Entiendo que
# puedo estar cargando mas de lo necesario, pero en estas circunstancias es mejor que sobre a que falte.

# Importar las librerias más que necesarias:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay
from sklearn.compose import make_column_selector, make_column_transformer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.compose import ColumnTransformer
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from sklearn.metrics import recall_score
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor

# A continuacion cargaremos las bases de datos. En esta primera instancia, cargaré mas de una porque
# quiero primero explorar los datos antes de tomar la decision sobre cuál trabajar.

# ===================== Primera base de datos: Suicidios en China ===================== #

df_China = pd.read_csv("/content/SuicideChina.csv")

df_China.head()

df_China.shape

df_China.duplicated().sum()

df_China.isna().sum()

df_China.info()

df_China["method"].value_counts()

df_China["Died"].value_counts()

df_China["Year"].value_counts()

df_China["Sex"].value_counts()

df_China["Age"].describe()

# Me resulta bastante interesante este dataset.

# ===================== Segunda base de datos: Suicidios Rate ===================== #

df_rate = pd.read_csv("/content/Suicide_Rate.csv")

df_rate.head()

df_rate["ParentLocation"].value_counts()

df_rate.loc[(df_rate['Location'] == "China")].describe()

# En este codigo que no me gustó tanto tengo 60 datos de China. Pero las demas columnas siento que
# no me proporcionan informacion relevante que pudiera añadir al database original.

# ===================== Segunda base de datos: Master (suicidios) ===================== #

df_master = pd.read_csv("/content/master.csv")

df_master.head()

df_master.info()

df_master.shape

df_master["country"].value_counts()

df_master.loc[(df_master['country'] == "China")].describe()

df_master["year"].value_counts()

# Pense que podia de alguna forma vincular esta base de datos con la de los suicidios en China, pero no me
# reconoce el pais China cuando intento buscarlo.

" ===================== Conclusion: Suicidios en China ===================== "

# Una vez explorada las bases de datos, la decision final es que se va a trabajar sobre la base que se
# titula Suicidios en China.

# ****************************************************************************************************************
# ===================================== Primera parte: Revision de la Data ===================================== #
# ****************************************************************************************************************

df = pd.read_csv("/content/SuicideChina.csv")

df.head(10)

df.shape

df["Hospitalised"].value_counts()

df["Died"].value_counts()

df["Urban"].value_counts()

df["Year"].value_counts()

df["Month"].value_counts()

df["Sex"].value_counts()

df["Age"].describe()

df["Education"].value_counts()

df["Occupation"].value_counts()

df["method"].value_counts()

df = df.drop(columns = ['Unnamed: 0', 'Person_ID'])

df.head()

def bar_plot(variable):
    """
    input: variable ex: "Sex"
    output: bar plot & Value count.
    """
    # get feature.
    var = df[variable]
    # count number of categorical variable(value / sample)
    varValue = var.value_counts()

    #visualization
    plt.figure(figsize = (9,3))
    plt.bar(varValue.index, varValue)
    plt.xticks(varValue.index, varValue.index.values, rotation = 45)
    plt.ylabel("Frequency")
    plt.title(variable)
    plt.show()
    print("{}: \n {}".format(variable, varValue))
category1 = ["Hospitalised","Died","Urban", "Sex", "Education", "Occupation","method", "Year", "Month"]
for c in category1:
    bar_plot(c)

plt.figure(figsize = (9,3))
plt.hist(df["Age"], bins = 50)
plt.xlabel("Edad")
plt.ylabel("Frecuencia")
plt.title("Distribucion de la edad de suicidios")
plt.grid()
plt.show()

df.head()

df["Hospitalised"]= df["Hospitalised"].replace("yes", 1)
df["Hospitalised"]= df["Hospitalised"].replace("no", 0)

df["Died"]= df["Died"].replace("yes", 1)
df["Died"]= df["Died"].replace("no", 0)

df["Urban"]= df["Urban"].replace("yes", 1)
df["Urban"]= df["Urban"].replace("no", 0)
df["Urban"]= df["Urban"].replace("unknown", 2)

df["Sex"]= df["Sex"].replace("female", 1)
df["Sex"]= df["Sex"].replace("male", 0)

df["Education"]= df["Education"].replace("iliterate", 0)
df["Education"]= df["Education"].replace("primary", 1)
df["Education"]= df["Education"].replace("Secondary", 2)
df["Education"]= df["Education"].replace("Tertiary", 3)
df["Education"]= df["Education"].replace("unknown", 4)

df["Occupation"]= df["Occupation"].replace("farming", 1)
df["Occupation"]= df["Occupation"].replace("household", 2)
df["Occupation"]= df["Occupation"].replace("others/unknown", "others")
df["Occupation"]= df["Occupation"].replace("others", 3)
df["Occupation"]= df["Occupation"].replace("professional", 4)
df["Occupation"]= df["Occupation"].replace("student", 5)
df["Occupation"]= df["Occupation"].replace("unemployed", 6)
df["Occupation"]= df["Occupation"].replace("business/service", 7)
df["Occupation"]= df["Occupation"].replace("worker", 8)
df["Occupation"]= df["Occupation"].replace("retiree", 9)

df["method"]= df["method"].replace("Pesticide", 1)
df["method"]= df["method"].replace("Hanging", 2)
df["method"]= df["method"].replace("Other poison", "Poison unspec")
df["method"]= df["method"].replace("Poison unspec", 3)
df["method"]= df["method"].replace("unspecified", 4)
df["method"]= df["method"].replace("Cutting", 5)
df["method"]= df["method"].replace("Drowning", 6)
df["method"]= df["method"].replace("Jumping", 7)
df["method"]= df["method"].replace("Others", 4)

df.head()

Correlacion = df.corr()
sns.heatmap(Correlacion, cmap='Greens', annot=True)
plt.title("Correlacion en base de datos de suicidios en Shandong")
plt.show()

df.info()

# Hasta aqui, terminamos de revisar los datos. Estos estan bien, son todos integer, no hay missing
# value, no hay NaN, no hay datos duplicados. Podemos aplicar modelos.

# ****************************************************************************************************************
# ================================ Segunda parte: Metrica y Modelos Aplicados ================================ #
# ****************************************************************************************************************

" ===================== Primer Modelo: Arboles de Decisión ===================== "

# Correré la base de datos de nuevo y todo lo modificado para comenzar a trabajar con el otro modelo.

# Cargamos la base de datps
df = pd.read_csv("/content/SuicideChina.csv")

# Eliminamos las columnas innecesarias
df = df.drop(columns = ['Unnamed: 0', 'Person_ID'])

# Cambiamos las columnas que son categoricas como valores numericos:

# Si fue Hospitalizado o no
df["Hospitalised"]= df["Hospitalised"].replace("yes", 1)
df["Hospitalised"]= df["Hospitalised"].replace("no", 0)

# Si falleció o no
df["Died"]= df["Died"].replace("yes", 1)
df["Died"]= df["Died"].replace("no", 0)

# si vivia en una zona urbana o rural
df["Urban"]= df["Urban"].replace("yes", 1)
df["Urban"]= df["Urban"].replace("no", 0)
df["Urban"]= df["Urban"].replace("unknown", 2)

# De qué genero era
df["Sex"]= df["Sex"].replace("female", 1)
df["Sex"]= df["Sex"].replace("male", 0)

# La educación que tenía
df["Education"]= df["Education"].replace("iliterate", 0)
df["Education"]= df["Education"].replace("primary", 1)
df["Education"]= df["Education"].replace("Secondary", 2)
df["Education"]= df["Education"].replace("Tertiary", 3)
df["Education"]= df["Education"].replace("unknown", 4)

# A qué se dedicaba
df["Occupation"]= df["Occupation"].replace("farming", 1)
df["Occupation"]= df["Occupation"].replace("household", 2)
df["Occupation"]= df["Occupation"].replace("others/unknown", "others")
df["Occupation"]= df["Occupation"].replace("others", 3)
df["Occupation"]= df["Occupation"].replace("professional", 4)
df["Occupation"]= df["Occupation"].replace("student", 5)
df["Occupation"]= df["Occupation"].replace("unemployed", 6)
df["Occupation"]= df["Occupation"].replace("business/service", 7)
df["Occupation"]= df["Occupation"].replace("worker", 8)
df["Occupation"]= df["Occupation"].replace("retiree", 9)

# El metodo que escogio para cometer suicidio (o intento de suicidio)
df["method"]= df["method"].replace("Pesticide", 1)
df["method"]= df["method"].replace("Hanging", 2)
df["method"]= df["method"].replace("Other poison", "Poison unspec")
df["method"]= df["method"].replace("Poison unspec", 3)
df["method"]= df["method"].replace("unspecified", 4)
df["method"]= df["method"].replace("Cutting", 5)
df["method"]= df["method"].replace("Drowning", 6)
df["method"]= df["method"].replace("Jumping", 7)
df["method"]= df["method"].replace("Others", 4)

# Variable objetivo: "Died"

y = df['Died']
X = df.drop(columns = 'Died')

# Hacemos el test train split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Aplicamos el modelo de arboles de decision

DecisionTreeClassifier,
dec_tree = DecisionTreeClassifier(random_state = 42)

# Ajustamos el modelo utilizando los datos de entrenamiento

dec_tree.fit(X_train, y_train)

# Predecir los valores objetivos para el conjunto de entrenamiento y de prueba.

train_preds = dec_tree.predict(X_train)
test_preds = dec_tree.predict(X_test)

# Y finalmente toca evaluar el modelo:

train_score = dec_tree.score(X_train, y_train)
test_score = dec_tree.score(X_test, y_test)
print(train_score)
print(test_score)

" ===================== Segundo Modelo: Random Forest ===================== "

# Correré la base de datos de nuevo y todo lo modificado para comenzar a trabajar con el otro modelo.

# Cargamos la base de datps
df = pd.read_csv("/content/SuicideChina.csv")

# Eliminamos las columnas innecesarias
df = df.drop(columns = ['Unnamed: 0', 'Person_ID'])

# Cambiamos las columnas que son categoricas como valores numericos:

# Si fue Hospitalizado o no
df["Hospitalised"]= df["Hospitalised"].replace("yes", 1)
df["Hospitalised"]= df["Hospitalised"].replace("no", 0)

# Si falleció o no
df["Died"]= df["Died"].replace("yes", 1)
df["Died"]= df["Died"].replace("no", 0)

# si vivia en una zona urbana o rural
df["Urban"]= df["Urban"].replace("yes", 1)
df["Urban"]= df["Urban"].replace("no", 0)
df["Urban"]= df["Urban"].replace("unknown", 2)

# De qué genero era
df["Sex"]= df["Sex"].replace("female", 1)
df["Sex"]= df["Sex"].replace("male", 0)

# La educación que tenía
df["Education"]= df["Education"].replace("iliterate", 0)
df["Education"]= df["Education"].replace("primary", 1)
df["Education"]= df["Education"].replace("Secondary", 2)
df["Education"]= df["Education"].replace("Tertiary", 3)
df["Education"]= df["Education"].replace("unknown", 4)

# A qué se dedicaba
df["Occupation"]= df["Occupation"].replace("farming", 1)
df["Occupation"]= df["Occupation"].replace("household", 2)
df["Occupation"]= df["Occupation"].replace("others/unknown", "others")
df["Occupation"]= df["Occupation"].replace("others", 3)
df["Occupation"]= df["Occupation"].replace("professional", 4)
df["Occupation"]= df["Occupation"].replace("student", 5)
df["Occupation"]= df["Occupation"].replace("unemployed", 6)
df["Occupation"]= df["Occupation"].replace("business/service", 7)
df["Occupation"]= df["Occupation"].replace("worker", 8)
df["Occupation"]= df["Occupation"].replace("retiree", 9)

# El metodo que escogio para cometer suicidio (o intento de suicidio)
df["method"]= df["method"].replace("Pesticide", 1)
df["method"]= df["method"].replace("Hanging", 2)
df["method"]= df["method"].replace("Other poison", "Poison unspec")
df["method"]= df["method"].replace("Poison unspec", 3)
df["method"]= df["method"].replace("unspecified", 4)
df["method"]= df["method"].replace("Cutting", 5)
df["method"]= df["method"].replace("Drowning", 6)
df["method"]= df["method"].replace("Jumping", 7)
df["method"]= df["method"].replace("Others", 4)

# Ordenen los datos en matriz de características y vector objetivo

X = df.drop(columns = 'Died')
y = df['Died']

# Dividan los datos para la validación

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Aplicamos el modelo

rf = RandomForestClassifier(random_state = 42)

# Vemos hiperparametros

rf.get_params()

# Aprende relacion entre X e y

rf.fit(X_train, y_train)

# Predecir los valores para y

rf.predict(X_test)

# Evaluar el modelo

rf_train_score = rf.score(X_train, y_train)
rf_test_score = rf.score(X_test, y_test)
print(rf_train_score)
print(rf_test_score)

# Afinaremos el modelo porque parece estar sobreajustado

rf_2 = RandomForestClassifier(max_depth = 2, random_state = 42)
rf_2.fit(X_train, y_train)
rf_2_train_score = rf_2.score(X_train, y_train)
rf_2_test_score = rf_2.score(X_test, y_test)
print(rf_2_train_score)
print(rf_2_test_score)

# Ahí se ve mejor. Y el resultado se parece mucho a con el modelo anterior.

" ===================== Tercero Modelo: Redes Neuronales ===================== "

# Correré la base de datos de nuevo y todo lo modificado para comenzar a trabajar con el otro modelo.

# Cargamos la base de datps
df = pd.read_csv("/content/SuicideChina.csv")

# Eliminamos las columnas innecesarias
df = df.drop(columns = ['Unnamed: 0', 'Person_ID'])

# Cambiamos las columnas que son categoricas como valores numericos:

# Si fue Hospitalizado o no
df["Hospitalised"]= df["Hospitalised"].replace("yes", 1)
df["Hospitalised"]= df["Hospitalised"].replace("no", 0)

# Si falleció o no
df["Died"]= df["Died"].replace("yes", 1)
df["Died"]= df["Died"].replace("no", 0)

# si vivia en una zona urbana o rural
df["Urban"]= df["Urban"].replace("yes", 1)
df["Urban"]= df["Urban"].replace("no", 0)
df["Urban"]= df["Urban"].replace("unknown", 2)

# De qué genero era
df["Sex"]= df["Sex"].replace("female", 1)
df["Sex"]= df["Sex"].replace("male", 0)

# La educación que tenía
df["Education"]= df["Education"].replace("iliterate", 0)
df["Education"]= df["Education"].replace("primary", 1)
df["Education"]= df["Education"].replace("Secondary", 2)
df["Education"]= df["Education"].replace("Tertiary", 3)
df["Education"]= df["Education"].replace("unknown", 4)

# A qué se dedicaba
df["Occupation"]= df["Occupation"].replace("farming", 1)
df["Occupation"]= df["Occupation"].replace("household", 2)
df["Occupation"]= df["Occupation"].replace("others/unknown", "others")
df["Occupation"]= df["Occupation"].replace("others", 3)
df["Occupation"]= df["Occupation"].replace("professional", 4)
df["Occupation"]= df["Occupation"].replace("student", 5)
df["Occupation"]= df["Occupation"].replace("unemployed", 6)
df["Occupation"]= df["Occupation"].replace("business/service", 7)
df["Occupation"]= df["Occupation"].replace("worker", 8)
df["Occupation"]= df["Occupation"].replace("retiree", 9)

# El metodo que escogio para cometer suicidio (o intento de suicidio)
df["method"]= df["method"].replace("Pesticide", 1)
df["method"]= df["method"].replace("Hanging", 2)
df["method"]= df["method"].replace("Other poison", "Poison unspec")
df["method"]= df["method"].replace("Poison unspec", 3)
df["method"]= df["method"].replace("unspecified", 4)
df["method"]= df["method"].replace("Cutting", 5)
df["method"]= df["method"].replace("Drowning", 6)
df["method"]= df["method"].replace("Jumping", 7)
df["method"]= df["method"].replace("Others", 4)

# Vamos a escalar los datos, para prepararlos para el modelo:

scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
X_scaled.head()

# Lo que hacemos ahora es crear el grafico del codo y el de silluete para saber el numero de clusteres optimos.

ks = range(2,10)
sils = []
inertias = []
for k in ks:
  kmeans = KMeans(n_clusters=k, random_state=42)
  kmeans.fit(X_scaled)
  sils.append(silhouette_score(X_scaled, kmeans.labels_))
  inertias.append(kmeans.inertia_)
fig, axes = plt.subplots(1,2, figsize=(15,5))
axes[0].plot(ks, sils)
axes[0].set_title('Silhouette Scores')
axes[0].set_xticks(ks)
axes[1].plot(ks, inertias)
axes[1].set_title('Inertia')
axes[1].set_xticks(ks);

# Ahora aplicamos el modelo kmeans con 4 clusteres:
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X_scaled)

# Agregamos el numero de clusteres al dataframe en su propia columna.
df['cluster'] = kmeans.labels_
df.head()

# Lo que hacemos ahora, es que agrupamos el dataframe de acuerdo a los clusters (los 4 escogidos) y hacemos que nos muestre el promedio de cada columna (esto es para
# tener una idea mas general de los valores).

cluster_groups = df.groupby('cluster', as_index=False).mean()
cluster_groups

# Graficamos los clusteres obtenidos.

fig, axes = plt.subplots(4,3, figsize = (30, 20))
axes = axes.ravel()
for i, col in enumerate(cluster_groups.columns[1:]):
  axes[i].bar(cluster_groups['cluster'], cluster_groups[col])
  axes[i].set_title(f'Mean {col}')

# Ahora vamos a las redes neuronales, antes era kmeans

# 1) Separación

X = df.drop(columns = 'Died')
y = df['Died']

# 2) Escalar

scaler = StandardScaler()
scaled_df = scaler.fit_transform(X)

sse = [] # Suma de errores cuadrados

for k in range(1, 23):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(df)
    sse.append(kmeans.inertia_)

# Graficar el gráfico del codo
plt.plot(range(1, 23), sse, marker='o')
plt.xlabel('Número de grupos (k)')
plt.ylabel('Suma de errores cuadrados')
plt.title('Gráfico del Codo')
plt.show();

sse = [] # Suma de errores cuadrados

for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(df)
    sse.append(kmeans.inertia_)

# Graficar el gráfico del codo
plt.plot(range(1, 10), sse, marker='o')
plt.xlabel('Número de grupos (k)')
plt.ylabel('Suma de errores cuadrados')
plt.title('Gráfico del Codo')
plt.show();

# Ahora podemos dividir los datos con el test train split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Creamos un pipeline. Ya escalamos los datos y este paso es bueno para evitar fugas.

transformer = make_pipeline(StandardScaler())

# Un pipeline dentro de otro añadiendole la regresion logistica.

logreg = LogisticRegression()
logreg_pipe = make_pipeline(transformer, logreg)
logreg_pipe.fit(X_train, y_train)

print('Training accuracy:', logreg_pipe.score(X_train, y_train))
print('Testing accuracy:', logreg_pipe.score(X_test, y_test))



# Debemos definir nuestra estructura de red. Tenemos 7 columnas, por lo que ese será el maximo de neuronas en la primera capa.

input_shape = X_train.shape[1]
input_shape

# En el examen, nos piden que utilicemos un modelo secuencial:

model = Sequential()

# Primera capa oculta
model.add(Dense(10, # Tengo 10 neuronas en mi primera capa
                input_dim = input_shape,
                activation = 'relu'))
# Capa de salida
model.add(Dense(1, activation = 'sigmoid'))

# Paso 2: Compilar

model.compile(loss = 'bce', optimizer = 'adam')

# Paso 3: Ajustar el modelo

history = model.fit(X_train, y_train,
                    validation_data = (X_test, y_test),
                    epochs=10)

# Visualizamos el resultado

plt.plot(history.history['loss'], label='Train loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.legend();

loss = model.evaluate(X_test, y_test)
loss

# Obtener las predicciones del modelo
y_pred = model.predict(X_test)

# Redondear las predicciones a la clase correspondiente
y_pred_rounded = np.round(y_pred)

# Calcular el recall
recall = recall_score(y_test, y_pred_rounded)

recall

# Obtener las predicciones del modelo
y_pred = model.predict(X_test)

# Redondear las predicciones a la clase correspondiente
y_pred_rounded = np.round(y_pred)

# Calcular la precisión
accuracy = accuracy_score(y_test, y_pred_rounded)

accuracy

# ===== Segunda version de redes neuronales =====

# Primera capa oculta
model.add(Dense(10, # Tengo 70 neuronas en mi primera capa
                input_dim = input_shape,
                activation = 'relu'))
# Segunda capa oculta
model.add(Dense(7, # Tengo 4 neuronas en mi segunda capa.
                activation = 'relu'))
# Tercera capa oculta
model.add(Dense(4, # Tengo 4 neuronas en mi segunda capa.
                activation = 'relu'))
# Capa de salida
model.add(Dense(1, activation = 'sigmoid'))

# Paso 2: Compilar

model.compile(loss = 'bce', optimizer = 'adam')

# Paso 3: Ajustar el modelo

history = model.fit(X_train, y_train,
                    validation_data = (X_test, y_test),
                    epochs=10)

# Visualizamos el resultado

plt.plot(history.history['loss'], label='Train loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.legend();

loss = model.evaluate(X_test, y_test)
loss

# Obtener las predicciones del modelo
y_pred = model.predict(X_test)

# Redondear las predicciones a la clase correspondiente
y_pred_rounded = np.round(y_pred)

# Calcular el recall
recall = recall_score(y_test, y_pred_rounded)

recall

# Obtener las predicciones del modelo
y_pred = model.predict(X_test)

# Redondear las predicciones a la clase correspondiente
y_pred_rounded = np.round(y_pred)

# Calcular la precisión
accuracy = accuracy_score(y_test, y_pred_rounded)

accuracy

# ===== Tercera version de redes neuronales =====

# Agregamos una regularizacion

model = keras.Sequential([
    layers.Dense(10,
                 input_dim=input_shape,
                 activation='relu',
                 kernel_regularizer=regularizers.l2(0.01)),
    layers.Dense(4,
                 activation='relu',
                 kernel_regularizer=regularizers.l2(0.01)),
    layers.Dense(1,
                 activation='sigmoid')
])

# Paso 2: Compilar

model.compile(loss = 'bce', optimizer = 'adam')

# Paso 3: Ajustar el modelo

history = model.fit(X_train, y_train,
                    validation_data = (X_test, y_test),
                    epochs=10)

# Visualizamos el resultado

plt.plot(history.history['loss'], label='Train loss')
plt.plot(history.history['val_loss'], label='Test Loss')
plt.legend();

loss = model.evaluate(X_test, y_test)
loss

# Obtener las predicciones del modelo
y_pred = model.predict(X_test)

# Redondear las predicciones a la clase correspondiente
y_pred_rounded = np.round(y_pred)

# Calcular el recall
recall = recall_score(y_test, y_pred_rounded)

recall

# Obtener las predicciones del modelo
y_pred = model.predict(X_test)

# Redondear las predicciones a la clase correspondiente
y_pred_rounded = np.round(y_pred)

# Calcular la precisión
accuracy = accuracy_score(y_test, y_pred_rounded)

accuracy